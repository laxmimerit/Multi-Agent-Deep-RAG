{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Agents Bootcamp - Financial Analysis with Gemini\n",
    "\n",
    "Complete guide to building production-ready agents with memory, middleware, and streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook covers:\n",
    "- **Short-term Memory**: Persist conversation state with SQLite\n",
    "- **Built-in Middleware**: Production patterns (summarization, limits, PII detection, todo tracking)\n",
    "- **Structured Output**: Type-safe agent responses with Pydantic\n",
    "- **Streaming Modes**: Real-time updates (`messages`, `updates`, `values`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Initialize model and tools for financial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from scripts import base_tools\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "# model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
    "\n",
    "system_prompt = \"\"\"You are a financial analyst specializing in tech stocks.\n",
    "Provide data-driven analysis with clear insights.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Agent\n",
    "\n",
    "Create a simple agent with tools but no memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search, base_tools.get_weather],\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "response = agent.invoke({\n",
    "    \"messages\": [HumanMessage(\"What's Apple's current stock price?\")]\n",
    "})\n",
    "\n",
    "print(response[\"messages\"][-1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"messages\"][-1].content_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\n",
    "    \"messages\": [\"What's Apple's current stock price?\"]\n",
    "})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Short-term Memory with SQLite\n",
    "\n",
    "Add conversation persistence using SQLite checkpointer. Agent remembers previous turns within a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"data/financial_agent.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn)\n",
    "\n",
    "agent_memory = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search, base_tools.get_weather],\n",
    "    system_prompt=system_prompt,\n",
    "    checkpointer=checkpointer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"session_1\"}}\n",
    "\n",
    "# First turn\n",
    "response = agent_memory.invoke({\n",
    "    \"messages\": [HumanMessage(\"Search for Apple's latest earnings\")]\n",
    "}, config)\n",
    "print(response[\"messages\"][-1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "\n",
    "agent_summary = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search, base_tools.get_weather],\n",
    "    system_prompt=system_prompt,\n",
    "    checkpointer=checkpointer,\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            trigger=[(\"tokens\", 4000), (\"messages\", 10)],\n",
    "            keep=(\"messages\", 4)\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Middleware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Middleware: Summarization\n",
    "\n",
    "Automatically summarize old messages when history grows too long.\n",
    "\n",
    "**Use Case**: Long conversations that exceed context windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "\n",
    "agent_summary = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search, base_tools.get_weather],\n",
    "    system_prompt=system_prompt,\n",
    "    checkpointer=checkpointer,\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=model,\n",
    "            trigger= (\"messages\", 10),\n",
    "            keep=(\"messages\", 5)\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Search for Apple stock\",\n",
    "    \"What about Microsoft?\",\n",
    "    \"Compare their P/E ratios\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    response = agent_summary.invoke({\"messages\": [HumanMessage(query)]}, config)\n",
    "    print(f\"Q: {query}\\nA: {response['messages'][-1].content}\\n\")\n",
    "\n",
    "state = agent_summary.get_state(config)\n",
    "print(f\"Total messages: {len(state.values['messages'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Middleware: Model and Tool Call Limit\n",
    "\n",
    "Limit the number of model calls per request to prevent runaway costs.\n",
    "\n",
    "**Exit Behaviors**: `\"end\"` (stop) or `\"continue\"` (proceed without model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import ModelCallLimitMiddleware\n",
    "from langchain.agents.middleware import ToolCallLimitMiddleware\n",
    "from langchain.agents.middleware import ModelFallbackMiddleware\n",
    "\n",
    "agent_limit = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search, base_tools.get_weather],\n",
    "    system_prompt=system_prompt,\n",
    "    middleware=[\n",
    "        ModelCallLimitMiddleware(\n",
    "            run_limit=2,\n",
    "            exit_behavior=\"end\"\n",
    "        ),\n",
    "        ToolCallLimitMiddleware(\n",
    "            run_limit=2,\n",
    "            exit_behavior=\"continue\"\n",
    "            \n",
    "        ),\n",
    "                ModelFallbackMiddleware(ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = agent_limit.invoke({\n",
    "    \"messages\": [HumanMessage(\"what is the latest apple stock price and what is the latest weather in mumbai?\")]\n",
    "})\n",
    "print(\"Limited calls:\", response[\"messages\"][-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Middleware: Guardrails and PII Detection\n",
    "\n",
    "Automatically detect and redact/mask personally identifiable information.\n",
    "\n",
    "**Strategies**: `\"redact\"` (remove), `\"mask\"` (replace with ***), `\"block\"` (prevent request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import PIIMiddleware\n",
    "\n",
    "agent_pii = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search, base_tools.get_weather],\n",
    "    system_prompt=system_prompt,\n",
    "    middleware=[\n",
    "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n",
    "        PIIMiddleware(\"credit_card\", strategy=\"mask\", apply_to_input=True),\n",
    "        PIIMiddleware(\"api_key\", detector=r\"sk-[a-zA-Z0-9]{32}\", strategy=\"block\", apply_to_input=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = agent_pii.invoke({\n",
    "    \"messages\": [HumanMessage(\"Hi my name is laxmi kant tiwari and my email is test@example.com for Apple updates\")]\n",
    "})\n",
    "print(response[\"messages\"][-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Middleware: Todo List\n",
    "\n",
    "Track and manage multi-step tasks within the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import TodoListMiddleware\n",
    "\n",
    "agent_todo = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search, base_tools.get_weather],\n",
    "    system_prompt=system_prompt,\n",
    "    checkpointer=checkpointer,\n",
    "    middleware=[TodoListMiddleware()]\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"todo_session\"}}\n",
    "\n",
    "response = agent_todo.invoke({\n",
    "    \"messages\": [HumanMessage(\"Analyze Apple revenue and compare competitors\")]\n",
    "}, config)\n",
    "print(response[\"messages\"][-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Structured Output\n",
    "\n",
    "Return type-safe Pydantic models from agent responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class FinancialAnalysis(BaseModel):\n",
    "    \"\"\"Structured financial analysis.\"\"\"\n",
    "    company: str = Field(description=\"Company name\")\n",
    "    stock_symbol: str = Field(description=\"Stock ticker\")\n",
    "    current_price: Optional[str] = Field(description=\"Current price\")\n",
    "    analysis: str = Field(description=\"Brief analysis\")\n",
    "    recommendation: str = Field(description=\"Buy/Hold/Sell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_structured = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search, base_tools.get_weather],\n",
    "    system_prompt=\"Provide structured financial analysis.\",\n",
    "    checkpointer=checkpointer,\n",
    "    response_format=FinancialAnalysis\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"structured_session\"}}\n",
    "\n",
    "response = agent_structured.invoke({\n",
    "    \"messages\": [HumanMessage(\"Analyze Apple stock\")]\n",
    "}, config)\n",
    "\n",
    "# Find the ToolMessage with structured output\n",
    "for msg in reversed(response[\"messages\"]):\n",
    "    if msg.name == \"FinancialAnalysis\":\n",
    "        # Parse the content to extract structured data\n",
    "        content = msg.content\n",
    "        \n",
    "        # Display in a clean format\n",
    "        print(\"=\" * 50)\n",
    "        print(\"STRUCTURED FINANCIAL ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Extract key-value pairs from the ToolMessage content\n",
    "        import re\n",
    "        matches = re.findall(r\"(\\w+)='([^']*)'\", content)\n",
    "        for key, value in matches:\n",
    "            print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        break\n",
    "else:\n",
    "    # Fallback to regular content\n",
    "    print(\"Response:\", response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Streaming: Messages Mode\n",
    "Three streaming modes for real-time agent updates:\n",
    "- **`messages`**: Stream individual messages as they're generated\n",
    "- **`updates`**: Stream state updates after each step\n",
    "- **`values`**: Stream complete state values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_stream = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search, base_tools.get_weather],\n",
    "    system_prompt=system_prompt,\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"stream_1\"}}\n",
    "\n",
    "print(\"\\n=== Stream: Messages ===\")\n",
    "for chunk in agent_stream.stream({\n",
    "    \"messages\": [HumanMessage(\"Quick Apple stock update\")]\n",
    "}, config):\n",
    "    print(chunk)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"stream_3\"}}\n",
    "\n",
    "print(\"\\n=== Stream: Values ===\")\n",
    "for chunk in agent_stream.stream({\n",
    "    \"messages\": [HumanMessage(\"Compare Apple and Microsoft\")]\n",
    "}, config, stream_mode=\"values\"):\n",
    "    if \"messages\" in chunk:\n",
    "        print(f\"State: {len(chunk['messages'])} messages\")\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
