{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion for Deep RAG\n",
    "\n",
    "In this notebook, we'll load extracted data into Qdrant vector database:\n",
    "\n",
    "- **Markdown**: Page-level chunks with metadata\n",
    "- **Tables**: Separate documents with context and page numbers\n",
    "- **Images**: Text descriptions embedded (generated in notebook 06-01b)\n",
    "- **Hybrid Search**: Dense (semantic) + Sparse (keyword) embeddings\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run notebook 06-01 first to extract PDFs\n",
    "- Run notebook 06-01b to generate image descriptions\n",
    "- Qdrant server running on localhost:6333\n",
    "- Google API key set in .env file\n",
    "\n",
    "**Output:**\n",
    "- Single Qdrant collection with all content types\n",
    "- Rich metadata for filtering (company, year, quarter, doc_type, page)\n",
    "- Deduplication using file hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode, FastEmbedSparse\n",
    "from langchain_core.documents import Document\n",
    "from qdrant_client import QdrantClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "MARKDOWN_DIR = \"data/rag-data/markdown\"\n",
    "TABLES_DIR = \"data/rag-data/tables\"\n",
    "IMAGES_DESC_DIR = \"data/rag-data/images_desc\"\n",
    "\n",
    "# Qdrant Configuration\n",
    "COLLECTION_NAME = \"financial_docs\"\n",
    "EMBEDDING_MODEL = \"models/gemini-embedding-001\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initialize Embeddings and Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create or Recreate Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    documents=[],  # Empty initialization\n",
    "    embedding=embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    url=\"http://localhost:6333\",\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    "    force_recreate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store._client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reuse from 06-01\ndef extract_metadata_from_filename(filename: str):\n    \"\"\"Extract metadata from filename.\"\"\"\n    \n    filename = filename.replace('.pdf', '').replace('.md', '')\n    parts = filename.split()\n\n    return {\n        'company_name': parts[0],\n        'doc_type': parts[1],\n        'fiscal_quarter': parts[2] if len(parts)==4 else None,\n        'fiscal_year': parts[-1]\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_file_hash(file_path: Path) -> str:\n",
    "    \"\"\"Compute SHA-256 hash for deduplication.\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = vector_store.client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        limit=10000,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "all_points[0][0].payload['metadata']['file_hash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_processed_hashes():\n    \"\"\"Get file hashes already in Qdrant.\"\"\"\n    \n    all_points = vector_store.client.scroll(\n        collection_name=COLLECTION_NAME,\n        limit=10000,\n        with_payload=True\n    )\n    \n    return set(\n        point.payload['metadata'].get('file_hash') \n        for point in all_points[0]\n    )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6. Ingestion Function"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def ingest_file(file_path: Path, processed_hashes: set):\n    \"\"\"Ingest any markdown file - automatically detects type and handles accordingly.\"\"\"\n    \n    file_hash = compute_file_hash(file_path)\n    if file_hash in processed_hashes:\n        return\n    \n    # Determine content type from path\n    path_parts = file_path.parts\n    if 'markdown' in path_parts:\n        content_type = 'text'\n        doc_name = file_path.name\n    elif 'tables' in path_parts:\n        content_type = 'table'\n        doc_name = file_path.parent.name + '.md'\n    elif 'images_desc' in path_parts:\n        content_type = 'image'\n        doc_name = file_path.parent.name + '.md'\n    else:\n        return  # Unknown type, skip\n    \n    # Read content and prepare metadata\n    content = file_path.read_text(encoding='utf-8')\n    file_metadata = extract_metadata_from_filename(doc_name)\n    \n    # Build base metadata (common for all documents)\n    base_metadata = file_metadata.copy()\n    base_metadata['content_type'] = content_type\n    base_metadata['file_hash'] = file_hash\n    base_metadata['source_file'] = file_path.name\n    \n    # Process content into (text, page_num) chunks\n    chunks = []\n    if content_type == 'text':\n        # Split markdown by page breaks\n        pages = content.split(\"<!-- page break -->\")\n        chunks = [(text.strip(), i) for i, text in enumerate(pages, start=1) if text.strip()]\n    else:\n        # Extract page number from filename (table_1_page_5.md or page_5.md)\n        page_num = None\n        if '_page_' in file_path.stem:\n            page_num = int(file_path.stem.split('_page_')[1])\n        chunks = [(content, page_num)]\n    \n    # Create documents with unified logic\n    documents = []\n    for text, page_num in chunks:\n        metadata = base_metadata.copy()\n        if page_num:\n            metadata['page'] = page_num\n        documents.append(Document(page_content=text, metadata=metadata))\n    \n    # Add to vector store\n    if documents:\n        vector_store.add_documents(documents)\n        processed_hashes.add(file_hash)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 7. Ingest All Data"
  },
  {
   "cell_type": "code",
   "source": "processed_hashes = get_processed_hashes()\n\n# Get all markdown files from all directories\nbase_path = Path(\"data/rag-data\")\nall_md_files = list(base_path.rglob(\"*.md\"))\n\n# Ingest everything with single function\nfor md_file in all_md_files:\n    ingest_file(md_file, processed_hashes)\n\nprint(\"All data ingested successfully\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 8. Verify Ingestion",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "collection_info = vector_store.client.get_collection(COLLECTION_NAME)\ncollection_info",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 9. Test Search",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test hybrid search\nquery = \"What is Amazon's revenue?\"\nresults = vector_store.similarity_search(query, k=5)\n\nresults",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}