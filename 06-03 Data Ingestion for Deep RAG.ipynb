{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion for Deep RAG\n",
    "\n",
    "In this notebook, we'll load extracted data into Qdrant vector database:\n",
    "\n",
    "- **Markdown**: Page-level chunks with metadata\n",
    "- **Tables**: Separate documents with context and page numbers\n",
    "- **Images**: Text descriptions embedded (generated in notebook 06-01b)\n",
    "- **Hybrid Search**: Dense (semantic) + Sparse (keyword) embeddings\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run notebook 06-01 first to extract PDFs\n",
    "- Run notebook 06-01b to generate image descriptions\n",
    "- Qdrant server running on localhost:6333\n",
    "- Google API key set in .env file\n",
    "\n",
    "**Output:**\n",
    "- Single Qdrant collection with all content types\n",
    "- Rich metadata for filtering (company, year, quarter, doc_type, page)\n",
    "- Deduplication using file hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode, FastEmbedSparse\n",
    "from langchain_core.documents import Document\n",
    "from qdrant_client import QdrantClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "MARKDOWN_DIR = \"data/rag-data/markdown\"\n",
    "TABLES_DIR = \"data/rag-data/tables\"\n",
    "IMAGES_DESC_DIR = \"data/rag-data/images_desc\"\n",
    "\n",
    "# Qdrant Configuration\n",
    "COLLECTION_NAME = \"financial_docs\"\n",
    "EMBEDDING_MODEL = \"models/gemini-embedding-001\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initialize Embeddings and Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create or Recreate Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    documents=[],  # Empty initialization\n",
    "    embedding=embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    url=\"http://localhost:6333\",\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    "    force_recreate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store._client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reuse from 06-01\ndef extract_metadata_from_filename(filename: str):\n    \"\"\"Extract metadata from filename.\"\"\"\n    \n    filename = filename.replace('.pdf', '').replace('.md', '')\n    parts = filename.split()\n\n    return {\n        'company_name': parts[0],\n        'doc_type': parts[1],\n        'fiscal_quarter': parts[2] if len(parts)==4 else None,\n        'fiscal_year': parts[-1]\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_file_hash(file_path: Path) -> str:\n",
    "    \"\"\"Compute SHA-256 hash for deduplication.\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = vector_store.client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        limit=10000,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "all_points[0][0].payload['metadata']['file_hash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_processed_hashes():\n    \"\"\"Get file hashes already in Qdrant.\"\"\"\n    \n    all_points = vector_store.client.scroll(\n        collection_name=COLLECTION_NAME,\n        limit=10000,\n        with_payload=True\n    )\n    \n    return set(\n        point.payload['metadata'].get('file_hash') \n        for point in all_points[0]\n    )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ingestion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def ingest_markdown_file(md_path: Path, processed_hashes: set):\n    \"\"\"Ingest markdown file split by pages.\"\"\"\n    \n    file_hash = compute_file_hash(md_path)\n    if file_hash in processed_hashes:\n        return 0\n    \n    markdown_text = md_path.read_text(encoding='utf-8')\n    pages = markdown_text.split(\"<!-- page break -->\")\n    \n    file_metadata = extract_metadata_from_filename(md_path.name)\n    \n    documents = []\n    for page_num, page_text in enumerate(pages, start=1):\n        page_content = page_text.strip()\n        if page_content:\n            metadata = file_metadata.copy()\n            metadata['content_type'] = 'text'\n            metadata['page'] = page_num\n            metadata['file_hash'] = file_hash\n            metadata['source_file'] = md_path.name\n            \n            documents.append(Document(page_content=page_content, metadata=metadata))\n    \n    if documents:\n        vector_store.add_documents(documents)\n        processed_hashes.add(file_hash)\n    \n    return len(documents)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def ingest_table_file(table_path: Path, doc_name: str, processed_hashes: set):\n    \"\"\"Ingest a single table file.\"\"\"\n    \n    file_hash = compute_file_hash(table_path)\n    if file_hash in processed_hashes:\n        return 0\n    \n    table_content = table_path.read_text(encoding='utf-8')\n    file_metadata = extract_metadata_from_filename(doc_name + '.md')\n    \n    # Extract table number and page from filename (table_1_page_5.md)\n    stem = table_path.stem\n    parts = stem.split('_')\n    table_num = int(parts[1])\n    page_num = int(parts[3]) if len(parts) >= 4 else None\n    \n    metadata = file_metadata.copy()\n    metadata['content_type'] = 'table'\n    metadata['table_number'] = table_num\n    metadata['page'] = page_num\n    metadata['file_hash'] = file_hash\n    metadata['source_file'] = table_path.name\n    \n    doc = Document(page_content=table_content, metadata=metadata)\n    vector_store.add_documents([doc])\n    processed_hashes.add(file_hash)\n    \n    return 1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def ingest_image_description(desc_path: Path, doc_name: str, processed_hashes: set):\n    \"\"\"Ingest image description file.\"\"\"\n    \n    file_hash = compute_file_hash(desc_path)\n    if file_hash in processed_hashes:\n        return 0\n    \n    description = desc_path.read_text(encoding='utf-8')\n    file_metadata = extract_metadata_from_filename(doc_name + '.md')\n    \n    # Extract page number from filename (page_5.md)\n    page_num = int(desc_path.stem.split('_')[1])\n    \n    metadata = file_metadata.copy()\n    metadata['content_type'] = 'image'\n    metadata['page'] = page_num\n    metadata['file_hash'] = file_hash\n    metadata['source_file'] = desc_path.name\n    \n    doc = Document(page_content=description, metadata=metadata)\n    vector_store.add_documents([doc])\n    processed_hashes.add(file_hash)\n    \n    return 1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_company_tables(company_dir: Path, processed_hashes: set) -> int:\n",
    "    \"\"\"Ingest all tables for a company.\"\"\"\n",
    "    table_count = 0\n",
    "    \n",
    "    for doc_dir in company_dir.iterdir():\n",
    "        if doc_dir.is_dir():\n",
    "            for table_file in doc_dir.glob(\"table_*.md\"):\n",
    "                table_count += ingest_table_file(table_file, doc_dir.name, processed_hashes)\n",
    "    \n",
    "    return table_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_company_image_descriptions(company_dir: Path, processed_hashes: set) -> int:\n",
    "    \"\"\"Ingest all image descriptions for a company.\"\"\"\n",
    "    desc_count = 0\n",
    "    \n",
    "    for doc_dir in company_dir.iterdir():\n",
    "        if doc_dir.is_dir():\n",
    "            for desc_file in doc_dir.glob(\"page_*.md\"):\n",
    "                desc_count += ingest_image_description(desc_file, doc_dir.name, processed_hashes)\n",
    "    \n",
    "    return desc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Process All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "processed_hashes = get_processed_hashes()\n\nmarkdown_path = Path(MARKDOWN_DIR)\nmd_files = list(markdown_path.rglob(\"*.md\"))\n\ntotal_pages = 0\nfor md_path in md_files:\n    total_pages += ingest_markdown_file(md_path, processed_hashes)\n\nprint(f\"Markdown: {total_pages} pages ingested\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "tables_path = Path(TABLES_DIR)\ncompany_dirs = [d for d in tables_path.iterdir() if d.is_dir()]\n\ntotal_tables = 0\nfor company_dir in company_dirs:\n    total_tables += ingest_company_tables(company_dir, processed_hashes)\n\nprint(f\"Tables: {total_tables} tables ingested\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "images_desc_path = Path(IMAGES_DESC_DIR)\ncompany_dirs = [d for d in images_desc_path.iterdir() if d.is_dir()]\n\ntotal_images = 0\nfor company_dir in company_dirs:\n    total_images += ingest_company_image_descriptions(company_dir, processed_hashes)\n\nprint(f\"Images: {total_images} descriptions ingested\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Verify Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_info = vector_store.client.get_collection(COLLECTION_NAME)\n",
    "collection_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Test Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hybrid search\n",
    "query = \"What is Amazon's revenue?\"\n",
    "results = vector_store.similarity_search(query, k=5)\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}