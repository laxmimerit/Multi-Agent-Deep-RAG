{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion for Deep RAG\n",
    "\n",
    "In this notebook, we'll load extracted data into Qdrant vector database:\n",
    "\n",
    "- **Markdown**: Page-level chunks with metadata\n",
    "- **Tables**: Separate documents with context and page numbers\n",
    "- **Images**: Text descriptions embedded (generated in notebook 06-01b)\n",
    "- **Hybrid Search**: Dense (semantic) + Sparse (keyword) embeddings\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run notebook 06-01 first to extract PDFs\n",
    "- Run notebook 06-01b to generate image descriptions\n",
    "- Qdrant server running on localhost:6333\n",
    "- Google API key set in .env file\n",
    "\n",
    "**Output:**\n",
    "- Single Qdrant collection with all content types\n",
    "- Rich metadata for filtering (company, year, quarter, doc_type, page)\n",
    "- Deduplication using file hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode, FastEmbedSparse\n",
    "from langchain_core.documents import Document\n",
    "from qdrant_client import QdrantClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "MARKDOWN_DIR = \"data/rag-data/markdown\"\n",
    "TABLES_DIR = \"data/rag-data/tables\"\n",
    "IMAGES_DESC_DIR = \"data/rag-data/images_desc\"\n",
    "\n",
    "# Qdrant Configuration\n",
    "COLLECTION_NAME = \"financial_docs\"\n",
    "EMBEDDING_MODEL = \"models/gemini-embedding-001\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initialize Embeddings and Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create or Recreate Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    documents=[],  # Empty initialization\n",
    "    embedding=embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    url=\"http://localhost:6333\",\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    "    force_recreate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store._client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_from_filename(filename: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract metadata from filename.\n",
    "    \n",
    "    Expected format: CompanyName DocType [Quarter] Year.md\n",
    "    Examples:\n",
    "        - Amazon 10-K 2024.md\n",
    "        - Amazon 10-Q Q1 2024.md\n",
    "    \"\"\"\n",
    "    name = filename.replace('.md', '').replace('.pdf', '')\n",
    "    parts = name.split()\n",
    "    \n",
    "    return {\n",
    "        'company_name': parts[0],\n",
    "        'doc_type': parts[1],\n",
    "        'fiscal_quarter': parts[2] if len(parts) == 4 else None,\n",
    "        'fiscal_year': int(parts[-1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_file_hash(file_path: Path) -> str:\n",
    "    \"\"\"Compute SHA-256 hash for deduplication.\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = vector_store.client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        limit=10000,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "all_points[0][0].payload['metadata']['file_hash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_hashes() -> set:\n",
    "    \"\"\"Get file hashes already in Qdrant.\"\"\"\n",
    "    all_points = vector_store.client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        limit=10000,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    hashes = set(\n",
    "        point.payload['metadata'].get('file_hash') \n",
    "        for point in all_points[0]\n",
    "    )\n",
    "    \n",
    "    print(f\"Already processed: {len(hashes)} files\")\n",
    "    return hashes\n",
    "\n",
    "hashes = get_processed_hashes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ingestion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_markdown_file(md_path: Path, processed_hashes: set):\n",
    "    \"\"\"Ingest markdown file split by pages.\"\"\"\n",
    "    file_hash = compute_file_hash(md_path)\n",
    "    if file_hash in processed_hashes:\n",
    "        print(f\"  [SKIP] {md_path.name}\")\n",
    "        return 0\n",
    "    \n",
    "    # Read and split by page breaks\n",
    "    markdown_text = md_path.read_text(encoding='utf-8')\n",
    "    pages = markdown_text.split(\"<!-- page break -->\")\n",
    "    \n",
    "    # Get metadata from filename\n",
    "    file_metadata = extract_metadata_from_filename(md_path.name)\n",
    "    \n",
    "    # Create documents for each page\n",
    "    documents = []\n",
    "    for page_num, page_text in enumerate(pages, start=1):\n",
    "        page_content = page_text.strip()\n",
    "        if page_content:\n",
    "            metadata = file_metadata.copy()\n",
    "            metadata['content_type'] = 'text'\n",
    "            metadata['page'] = page_num\n",
    "            metadata['file_hash'] = file_hash\n",
    "            metadata['source_file'] = md_path.name\n",
    "            \n",
    "            documents.append(Document(page_content=page_content, metadata=metadata))\n",
    "    \n",
    "    # Add to vector store\n",
    "    if documents:\n",
    "        vector_store.add_documents(documents)\n",
    "        processed_hashes.add(file_hash)\n",
    "        print(f\"  ✓ {md_path.name} ({len(documents)} pages)\")\n",
    "    \n",
    "    return len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_table_file(table_path: Path, doc_name: str, processed_hashes: set):\n",
    "    \"\"\"Ingest a single table file.\"\"\"\n",
    "    file_hash = compute_file_hash(table_path)\n",
    "    if file_hash in processed_hashes:\n",
    "        return 0\n",
    "    \n",
    "    # Read table content\n",
    "    table_content = table_path.read_text(encoding='utf-8')\n",
    "    \n",
    "    # Extract metadata from filename\n",
    "    file_metadata = extract_metadata_from_filename(doc_name + '.md')\n",
    "    \n",
    "    # Extract table number and page number from filename\n",
    "    stem = table_path.stem\n",
    "    parts = stem.split('_')\n",
    "    table_num = int(parts[1])\n",
    "    page_num = int(parts[3]) if len(parts) >= 4 else None\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = file_metadata.copy()\n",
    "    metadata['content_type'] = 'table'\n",
    "    metadata['table_number'] = table_num\n",
    "    metadata['page'] = page_num\n",
    "    metadata['file_hash'] = file_hash\n",
    "    metadata['source_file'] = table_path.name\n",
    "    \n",
    "    # Create document and add to vector store\n",
    "    doc = Document(page_content=table_content, metadata=metadata)\n",
    "    vector_store.add_documents([doc])\n",
    "    processed_hashes.add(file_hash)\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_image_description(desc_path: Path, doc_name: str, processed_hashes: set):\n",
    "    \"\"\"Ingest image description file.\"\"\"\n",
    "    file_hash = compute_file_hash(desc_path)\n",
    "    if file_hash in processed_hashes:\n",
    "        return 0\n",
    "    \n",
    "    # Read description\n",
    "    description = desc_path.read_text(encoding='utf-8')\n",
    "    \n",
    "    # Extract metadata from filename\n",
    "    file_metadata = extract_metadata_from_filename(doc_name + '.md')\n",
    "    \n",
    "    # Extract page number from filename (page_5.md)\n",
    "    page_num = int(desc_path.stem.split('_')[1])\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = file_metadata.copy()\n",
    "    metadata['content_type'] = 'image'\n",
    "    metadata['page'] = page_num\n",
    "    metadata['file_hash'] = file_hash\n",
    "    metadata['source_file'] = desc_path.name\n",
    "    \n",
    "    # Create document and add to vector store\n",
    "    doc = Document(page_content=description, metadata=metadata)\n",
    "    vector_store.add_documents([doc])\n",
    "    processed_hashes.add(file_hash)\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_company_tables(company_dir: Path, processed_hashes: set) -> int:\n",
    "    \"\"\"Ingest all tables for a company.\"\"\"\n",
    "    table_count = 0\n",
    "    \n",
    "    for doc_dir in company_dir.iterdir():\n",
    "        if doc_dir.is_dir():\n",
    "            for table_file in doc_dir.glob(\"table_*.md\"):\n",
    "                table_count += ingest_table_file(table_file, doc_dir.name, processed_hashes)\n",
    "    \n",
    "    return table_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_company_image_descriptions(company_dir: Path, processed_hashes: set) -> int:\n",
    "    \"\"\"Ingest all image descriptions for a company.\"\"\"\n",
    "    desc_count = 0\n",
    "    \n",
    "    for doc_dir in company_dir.iterdir():\n",
    "        if doc_dir.is_dir():\n",
    "            for desc_file in doc_dir.glob(\"page_*.md\"):\n",
    "                desc_count += ingest_image_description(desc_file, doc_dir.name, processed_hashes)\n",
    "    \n",
    "    return desc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Process All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get already processed files\n",
    "processed_hashes = get_processed_hashes()\n",
    "\n",
    "# Process markdown files\n",
    "print(\"\\n=== Ingesting Markdown Files ===\")\n",
    "markdown_path = Path(MARKDOWN_DIR)\n",
    "md_files = list(markdown_path.rglob(\"*.md\"))\n",
    "print(f\"Found {len(md_files)} markdown files\\n\")\n",
    "\n",
    "total_pages = 0\n",
    "for idx, md_path in enumerate(md_files, 1):\n",
    "    print(f\"[{idx}/{len(md_files)}]\", end=\" \")\n",
    "    total_pages += ingest_markdown_file(md_path, processed_hashes)\n",
    "\n",
    "print(f\"\\nTotal pages ingested: {total_pages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process tables\n",
    "print(\"\\n=== Ingesting Tables ===\")\n",
    "tables_path = Path(TABLES_DIR)\n",
    "company_dirs = [d for d in tables_path.iterdir() if d.is_dir()]\n",
    "print(f\"Found {len(company_dirs)} companies\\n\")\n",
    "\n",
    "total_tables = 0\n",
    "for idx, company_dir in enumerate(company_dirs, 1):\n",
    "    print(f\"[{idx}/{len(company_dirs)}] {company_dir.name}...\", end=\" \")\n",
    "    count = ingest_company_tables(company_dir, processed_hashes)\n",
    "    total_tables += count\n",
    "    print(f\"✓ {count} tables\")\n",
    "\n",
    "print(f\"\\nTotal tables ingested: {total_tables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process image descriptions\n",
    "print(\"\\n=== Ingesting Image Descriptions ===\")\n",
    "images_desc_path = Path(IMAGES_DESC_DIR)\n",
    "company_dirs = [d for d in images_desc_path.iterdir() if d.is_dir()]\n",
    "print(f\"Found {len(company_dirs)} companies\\n\")\n",
    "\n",
    "total_images = 0\n",
    "for idx, company_dir in enumerate(company_dirs, 1):\n",
    "    print(f\"[{idx}/{len(company_dirs)}] {company_dir.name}...\", end=\" \")\n",
    "    count = ingest_company_image_descriptions(company_dir, processed_hashes)\n",
    "    total_images += count\n",
    "    print(f\"✓ {count} images\")\n",
    "\n",
    "print(f\"\\nTotal image descriptions ingested: {total_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Verify Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_info = vector_store.client.get_collection(COLLECTION_NAME)\n",
    "collection_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Test Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hybrid search\n",
    "query = \"What is Amazon's revenue?\"\n",
    "results = vector_store.similarity_search(query, k=5)\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
