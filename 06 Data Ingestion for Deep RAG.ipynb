{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced RAG - Data Ingestion Pipeline for PageRAG\n",
    "### Page-wise Document Processing with Gemini Embeddings and Qdrant\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Extract text from PDFs page by page\n",
    "- Extract metadata from filename\n",
    "- Store in Qdrant with rich metadata\n",
    "- Use Gemini embeddings\n",
    "\n",
    "**Use Cases:**\n",
    "1. Financial Analysis: Process SEC filings (10-K, 10-Q)\n",
    "2. Legal: Organize contracts and case documents\n",
    "3. Research: Index academic papers\n",
    "4. Enterprise: Searchable document repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode, FastEmbedSparse\n",
    "from langchain_core.documents import Document\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, SparseVectorParams, SparseIndexParams\n",
    "\n",
    "from docling.document_converter import DocumentConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = \"data\"\n",
    "QDRANT_PATH = \"./qdrant_financial_db\"\n",
    "COLLECTION_NAME = \"financial_docs\"\n",
    "EMBEDDING_MODEL = \"models/gemini-embedding-001\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Gemini Embeddings, BM25, and Qdrant\n",
    "\n",
    "**Hybrid Retrieval**: Combines dense (semantic) and sparse (keyword) search for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 14:20:29,225 - INFO - HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "2025-12-12 14:20:29,233 - INFO - HTTP Request: GET http://localhost:6333/collections/financial_docs/exists \"HTTP/1.1 200 OK\"\n",
      "2025-12-12 14:20:29,780 - INFO - HTTP Request: PUT http://localhost:6333/collections/financial_docs \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Dense embeddings (Gemini)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "\n",
    "# Sparse embeddings (BM25)\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "# Initialize vector store with hybrid retrieval\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    documents=[],\n",
    "    embedding=embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    url=\"http://localhost:6333\",\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    "    force_recreate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Extraction from Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_from_filename(filename: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract metadata from filename.\n",
    "    \n",
    "    Expected format: {company} {doc_type} {quarter} {year}.pdf\n",
    "    Examples:\n",
    "    - amazon 10-k 2024.pdf\n",
    "    - amazon 10-q q1 2024.pdf\n",
    "    \n",
    "    Returns:\n",
    "        dict with company_name, doc_type, fiscal_year, fiscal_quarter\n",
    "    \"\"\"\n",
    "    name = filename.replace('.pdf', '')\n",
    "    parts = name.split()\n",
    "    \n",
    "    metadata = {}\n",
    "    \n",
    "    if len(parts) == 4:\n",
    "        metadata['fiscal_quarter'] = parts[2]\n",
    "        metadata['fiscal_year'] = int(parts[3])\n",
    "    else:\n",
    "        metadata['fiscal_quarter'] = None\n",
    "        metadata['fiscal_year'] = int(parts[2])\n",
    "    \n",
    "    metadata['company_name'] = parts[0]\n",
    "    metadata['doc_type'] = parts[1]\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fiscal_quarter': None,\n",
       " 'fiscal_year': 2023,\n",
       " 'company_name': 'amazon',\n",
       " 'doc_type': '10-k'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_metadata_from_filename('amazon 10-k 2023.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fiscal_quarter': 'q1',\n",
       " 'fiscal_year': 2024,\n",
       " 'company_name': 'amazon',\n",
       " 'doc_type': '10-q'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_metadata_from_filename('amazon 10-q q1 2024.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text from PDF Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_pages(pdf_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract text from each page of PDF.\n",
    "    \n",
    "    Returns:\n",
    "        List of page texts\n",
    "    \"\"\"\n",
    "    converter = DocumentConverter()\n",
    "    result = converter.convert(pdf_path)\n",
    "    \n",
    "    page_break = \"<!-- page break -->\"\n",
    "    markdown_text = result.document.export_to_markdown(page_break_placeholder=page_break)\n",
    "    \n",
    "    pages = markdown_text.split(page_break)\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 14:21:55,006 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-12 14:21:55,067 - INFO - Going to convert document batch...\n",
      "2025-12-12 14:21:55,068 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
      "2025-12-12 14:21:55,094 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-12 14:21:55,107 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-12 14:21:55,125 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-12 14:21:55,153 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-12 14:21:56,763 - INFO - Accelerator device: 'cuda:0'\n",
      "\u001b[32m[INFO] 2025-12-12 14:21:56,773 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-12 14:21:56,791 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-12 14:21:56,792 [RapidOCR] main.py:53: Using C:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-12 14:21:56,876 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-12 14:21:56,879 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-12 14:21:56,880 [RapidOCR] main.py:53: Using C:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-12 14:21:56,903 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-12 14:21:56,914 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-12 14:21:56,924 [RapidOCR] main.py:53: Using C:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "2025-12-12 14:21:57,014 - INFO - Auto OCR model selected rapidocr with onnxruntime.\n",
      "2025-12-12 14:21:57,031 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-12-12 14:22:21,620 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-12-12 14:22:21,983 - INFO - Processing document amazon 10-q q1 2024.pdf\n"
     ]
    }
   ],
   "source": [
    "pages = extract_pdf_pages('data/rag-data/amazon/amazon 10-q q1 2024.pdf')\n",
    "print(f\"Total pages: {len(pages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Hash for Duplicate Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_file_hash(file_path: str) -> str:\n",
    "    \"\"\"Compute SHA-256 hash of file content.\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_file_hash('data/data-rag/amazon/amazon 10-q q1 2024.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Processed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get already processed files from Qdrant\n",
    "all_points = vector_store.client.scroll(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    limit=10000,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "processed_hashes = set(\n",
    "    point.payload.get('file_hash') \n",
    "    for point in all_points[0] \n",
    "    if point.payload.get('file_hash')\n",
    ")\n",
    "\n",
    "print(f\"Already processed: {len(processed_hashes)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_docs_in_vectordb(pdf_path: Path):\n",
    "    \"\"\"Process and ingest PDF into Qdrant vector store.\"\"\"\n",
    "    print(f\"Processing: {pdf_path.name}\")\n",
    "    \n",
    "    file_hash = compute_file_hash(pdf_path)\n",
    "    if file_hash in processed_hashes:\n",
    "        print(f\"[SKIP] Already processed: {pdf_path.name}\")\n",
    "        return\n",
    "    \n",
    "    pages = extract_pdf_pages(str(pdf_path))\n",
    "    file_metadata = extract_metadata_from_filename(pdf_path.name)\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    for page_num, page_text in enumerate(pages, start=1):\n",
    "        metadata = file_metadata.copy()\n",
    "        metadata['page'] = page_num\n",
    "        metadata['file_hash'] = file_hash\n",
    "        metadata['source_file'] = pdf_path.name\n",
    "        \n",
    "        doc = Document(page_content=page_text, metadata=metadata)\n",
    "        documents.append(doc)\n",
    "    \n",
    "    vector_store.add_documents(documents=documents)\n",
    "    processed_hashes.add(file_hash)\n",
    "    \n",
    "    print(f\"[DONE] Ingested {len(documents)} pages from {pdf_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process All PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(DATA_DIR)\n",
    "pdf_files = list(data_path.rglob(\"*.pdf\"))\n",
    "print(f\"Found {len(pdf_files)} PDF files\")\n",
    "pdf_files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf_path in pdf_files:\n",
    "    ingest_docs_in_vectordb(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_info = vector_store.client.get_collection(COLLECTION_NAME)\n",
    "print(f\"Total documents in Qdrant: {collection_info.points_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid search example (dense + sparse)\n",
    "results = vector_store.similarity_search(\n",
    "    \"What is Tesla's revenue for Q1 2024?\",\n",
    "    k=3\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Company: {doc.metadata.get('company_name')}\")\n",
    "    print(f\"Document: {doc.metadata.get('doc_type')}\")\n",
    "    print(f\"Year: {doc.metadata.get('fiscal_year')}\")\n",
    "    print(f\"Page: {doc.metadata.get('page')}\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
