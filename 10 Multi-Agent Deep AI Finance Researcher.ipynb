{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b48b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ebd3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import ollama\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import InjectedState\n",
    "\n",
    "from scripts.file_tools import (\n",
    "    DeepAgentState,\n",
    "    ls,\n",
    "    read_file,\n",
    "    write_file,\n",
    "    cleanup_files,\n",
    ")\n",
    "from scripts.prompts import (\n",
    "    ORCHESTRATOR_PROMPT,\n",
    "    RESEARCHER_PROMPT,\n",
    "    EDITOR_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e9d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Web Search Tool\n",
    "# -------------------------\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Perform a live web search using Ollama Cloud Web Search API.\n",
    "\n",
    "    Input:\n",
    "        query: search query string\n",
    "\n",
    "    Output:\n",
    "        JSON string of top results (max_results=2).\n",
    "    \"\"\"\n",
    "    response = ollama.web_search(query, max_results=2)\n",
    "    response = response.results\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4adced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = web_search.invoke({\"query\": \"Latest global news\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e54aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# LLM\n",
    "# -------------------------\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab05c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Worker Agents: Researcher & Editor\n",
    "# -------------------------\n",
    "\n",
    "researcher_tools = [ls, write_file, read_file, web_search]\n",
    "researcher_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=researcher_tools,\n",
    "    system_prompt=RESEARCHER_PROMPT,\n",
    "    state_schema=DeepAgentState,\n",
    ")\n",
    "\n",
    "editor_tools = [ls, read_file, write_file, cleanup_files]\n",
    "editor_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=editor_tools,\n",
    "    system_prompt=EDITOR_PROMPT,\n",
    "    state_schema=DeepAgentState,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Orchestrator Tools (call other agents)\n",
    "# -------------------------\n",
    "from typing import Annotated\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from langchain_core.tools import InjectedToolCallId\n",
    "from langgraph.types import Command\n",
    "\n",
    "@tool\n",
    "def run_researcher(\n",
    "    theme_id: int,\n",
    "    thematic_question: str,\n",
    "    state: Annotated[DeepAgentState, InjectedState],\n",
    "    max_retries: int = 2\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run a single Research agent for ONE thematic question.\n",
    "\n",
    "    Args:\n",
    "        theme_id: The theme number (1, 2, 3, etc.)\n",
    "        thematic_question: The specific thematic question to research\n",
    "        state: Injected agent state providing user_id/thread_id\n",
    "        max_retries: Number of retry attempts if researcher fails\n",
    "\n",
    "    The Researcher will:\n",
    "    - Receive ONE thematic question as input\n",
    "    - Break it into 2-4 focused search queries\n",
    "    - Use web_search to gather information\n",
    "    - Write files to researcher/ folder with hash-based names:\n",
    "        * researcher/<hash>_theme.md (research findings)\n",
    "        * researcher/<hash>_sources.txt (raw sources)\n",
    "\n",
    "    Returns a short status string for the orchestrator.\n",
    "    \"\"\"\n",
    "    from scripts.file_tools import generate_hash\n",
    "    \n",
    "    # Generate hash for unique file naming\n",
    "    file_hash = generate_hash(f\"{theme_id}_{thematic_question}\")\n",
    "    \n",
    "    # Build sub-state with theme-specific context\n",
    "    sub_state: DeepAgentState = {\n",
    "        \"messages\": state[\"messages\"] + [\n",
    "            AIMessage(content=f\"[THEME {theme_id}] Research this question: {thematic_question}\\n\\n\"\n",
    "                             f\"File hash: {file_hash}\\n\"\n",
    "                             f\"Save your findings to: researcher/{file_hash}_theme.md\\n\"\n",
    "                             f\"Save your sources to: researcher/{file_hash}_sources.txt\")\n",
    "        ],\n",
    "        \"user_id\": state.get(\"user_id\"),\n",
    "        \"thread_id\": state.get(\"thread_id\"),\n",
    "    }\n",
    "    \n",
    "    # Execute researcher with retry logic\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            researcher_agent.invoke(sub_state)\n",
    "            return f\"✓ Theme {theme_id} research completed (hash: {file_hash})\"\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries:\n",
    "                print(f\"⚠ Theme {theme_id} failed (attempt {attempt + 1}/{max_retries + 1}), retrying...\")\n",
    "                continue\n",
    "            else:\n",
    "                return f\"✗ Theme {theme_id} failed after {max_retries + 1} attempts: {str(e)}\"\n",
    "    \n",
    "    return f\"✓ Theme {theme_id} research completed (hash: {file_hash})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848a9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def write_research_plan(\n",
    "    thematic_questions: list[str],\n",
    "    state: Annotated[DeepAgentState, InjectedState],\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId],\n",
    ") -> Command:\n",
    "    \"\"\"\n",
    "    Write the high-level research plan with major thematic questions.\n",
    "\n",
    "    Args:\n",
    "        thematic_questions: List of 3-5 major thematic questions that break down\n",
    "                           the user's query. These guide the Researcher agent.\n",
    "        state: Injected agent state providing user_id/thread_id.\n",
    "        tool_call_id: Tool call ID for attaching a ToolMessage.\n",
    "\n",
    "    The research_plan.md will be read by the Researcher to guide tactical research.\n",
    "\n",
    "    Returns:\n",
    "        Command with a ToolMessage confirming the plan was written.\n",
    "    \"\"\"\n",
    "    from scripts.file_tools import _disk_path\n",
    "    from langchain_core.messages import ToolMessage\n",
    "    from langgraph.types import Command\n",
    "    \n",
    "    # Build content for research_plan.md\n",
    "    content = \"# Research Plan\\n\\n\"\n",
    "    content += \"## User Query\\n\"\n",
    "    # Extract user query from messages\n",
    "    user_msg = [m for m in state[\"messages\"] if hasattr(m, 'content')]\n",
    "    if user_msg:\n",
    "        content += f\"{user_msg[-1].content}\\n\\n\"\n",
    "    \n",
    "    content += \"## Thematic Questions\\n\\n\"\n",
    "    for i, question in enumerate(thematic_questions, 1):\n",
    "        content += f\"{i}. {question}\\n\"\n",
    "    \n",
    "    # Write to disk\n",
    "    path = _disk_path(state, \"research_plan.md\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    msg = f\"[RESEARCH PLAN WRITTEN] research_plan.md with {len(thematic_questions)} thematic questions -> {path}\"\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [ToolMessage(msg, tool_call_id=tool_call_id)]\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "@tool\n",
    "def run_editor(\n",
    "    state: Annotated[DeepAgentState, InjectedState]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run the Editor agent for this user/thread.\n",
    "\n",
    "    The Editor will:\n",
    "    - read research_plan.md, theme files, sources.txt\n",
    "    - write the final answer to report.md\n",
    "\n",
    "    Returns a short status string for the orchestrator.\n",
    "    \"\"\"\n",
    "    sub_state: DeepAgentState = {\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"user_id\": state.get(\"user_id\"),\n",
    "        \"thread_id\": state.get(\"thread_id\"),\n",
    "    }\n",
    "    editor_agent.invoke(sub_state)\n",
    "    return \"Editor completed. Final report is written to report.md.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf2338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Orchestrator Agent\n",
    "# -------------------------\n",
    "\n",
    "orchestrator_tools = [\n",
    "    write_research_plan,  # strategic planning\n",
    "    run_researcher,       # tactical research\n",
    "    run_editor,           # synthesis\n",
    "    cleanup_files,        # resetting workspace\n",
    "]\n",
    "\n",
    "orchestrator_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=orchestrator_tools,\n",
    "    system_prompt=ORCHESTRATOR_PROMPT,\n",
    "    state_schema=DeepAgentState,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a490f13c",
   "metadata": {},
   "source": [
    "\n",
    "### Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "state = {\n",
    "        \"messages\": [HumanMessage(content=\"Tell me a joke about computers.\")],\n",
    "        \"user_id\": \"user_123\",\n",
    "        \"thread_id\": \"thread_mcp_001\",\n",
    "    }\n",
    "\n",
    "result = orchestrator_agent.invoke(state)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "state = {\n",
    "        \"messages\": [HumanMessage(content=\"Do a detailed, well-structured analysis of MCP including history\")],\n",
    "        \"user_id\": \"user_123\",\n",
    "        \"thread_id\": \"thread_mcp_001\",\n",
    "    }\n",
    "\n",
    "result = orchestrator_agent.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7496c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae5976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "state = {\n",
    "        \"messages\": [HumanMessage(content=\"Do a detailed, well-structured comparison of MCP and API.\")],\n",
    "        \"user_id\": \"user_123\",\n",
    "        \"thread_id\": \"thread_mcp_001\",\n",
    "    }\n",
    "\n",
    "result = orchestrator_agent.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4447a621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1d418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
