{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep AI Finance Researcher\n",
    "\n",
    "Hierarchical multi-agent system with Orchestrator, Researcher, and Editor agents for deep financial research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "import sqlite3\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool, InjectedToolCallId\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from langgraph.types import Command\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "from scripts.rag_tools import hybrid_search, live_finance_researcher\n",
    "from scripts.file_tools import (\n",
    "    DeepAgentState,\n",
    "    ls,\n",
    "    read_file,\n",
    "    write_file,\n",
    "    cleanup_files,\n",
    "    generate_hash,\n",
    "    _disk_path\n",
    ")\n",
    "from scripts.prompts import (\n",
    "    ORCHESTRATOR_PROMPT,\n",
    "    RESEARCHER_PROMPT,\n",
    "    EDITOR_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model='gemini-2.5-flash', temperature=0.7)\n",
    "\n",
    "conn = sqlite3.connect(\"data/deep_finance_researcher.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn=conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Worker Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Researcher Agent - uses RAG and live finance tools\n",
    "researcher_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[ls, write_file, read_file, hybrid_search, live_finance_researcher],\n",
    "    system_prompt=RESEARCHER_PROMPT,\n",
    "    state_schema=DeepAgentState,\n",
    ")\n",
    "\n",
    "# Editor Agent\n",
    "editor_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[ls, read_file, write_file, cleanup_files],\n",
    "    system_prompt=EDITOR_PROMPT,\n",
    "    state_schema=DeepAgentState,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Orchestrator Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def run_researcher(\n",
    "    theme_id: int,\n",
    "    thematic_question: str,\n",
    "    state: Annotated[DeepAgentState, InjectedState],\n",
    "    max_retries: int = 2\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run a single Research agent for ONE thematic question.\n",
    "\n",
    "    Args:\n",
    "        theme_id: The theme number (1, 2, 3, etc.)\n",
    "        thematic_question: The specific thematic question to research\n",
    "        state: Injected agent state\n",
    "        max_retries: Number of retry attempts\n",
    "\n",
    "    Returns:\n",
    "        Status string for the orchestrator\n",
    "    \"\"\"\n",
    "    file_hash = generate_hash(f\"{theme_id}_{thematic_question}\")\n",
    "    \n",
    "    sub_state: DeepAgentState = {\n",
    "        \"messages\": state[\"messages\"] + [\n",
    "            AIMessage(content=f\"[THEME {theme_id}] Research this question: {thematic_question}\\n\\n\"\n",
    "                             f\"File hash: {file_hash}\\n\"\n",
    "                             f\"Save your findings to: researcher/{file_hash}_theme.md\\n\"\n",
    "                             f\"Save your sources to: researcher/{file_hash}_sources.txt\")\n",
    "        ],\n",
    "        \"user_id\": state.get(\"user_id\"),\n",
    "        \"thread_id\": state.get(\"thread_id\"),\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            researcher_agent.invoke(sub_state)\n",
    "            return f\"✓ Theme {theme_id} research completed (hash: {file_hash})\"\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries:\n",
    "                print(f\"⚠ Theme {theme_id} failed (attempt {attempt + 1}/{max_retries + 1}), retrying...\")\n",
    "                continue\n",
    "            else:\n",
    "                return f\"✗ Theme {theme_id} failed after {max_retries + 1} attempts: {str(e)}\"\n",
    "    \n",
    "    return f\"✓ Theme {theme_id} research completed (hash: {file_hash})\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def write_research_plan(\n",
    "    thematic_questions: list[str],\n",
    "    state: Annotated[DeepAgentState, InjectedState],\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId],\n",
    ") -> Command:\n",
    "    \"\"\"\n",
    "    Write the high-level research plan with major thematic questions.\n",
    "\n",
    "    Args:\n",
    "        thematic_questions: List of 3-5 major thematic questions\n",
    "        state: Injected agent state\n",
    "        tool_call_id: Tool call ID\n",
    "\n",
    "    Returns:\n",
    "        Command with ToolMessage confirming the plan was written\n",
    "    \"\"\"\n",
    "    content = \"# Research Plan\\n\\n\"\n",
    "    content += \"## User Query\\n\"\n",
    "    user_msg = [m for m in state[\"messages\"] if hasattr(m, 'content')]\n",
    "    if user_msg:\n",
    "        content += f\"{user_msg[-1].content}\\n\\n\"\n",
    "    \n",
    "    content += \"## Thematic Questions\\n\\n\"\n",
    "    for i, question in enumerate(thematic_questions, 1):\n",
    "        content += f\"{i}. {question}\\n\"\n",
    "    \n",
    "    path = _disk_path(state, \"research_plan.md\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    msg = f\"[RESEARCH PLAN WRITTEN] research_plan.md with {len(thematic_questions)} thematic questions\"\n",
    "    return Command(update={\"messages\": [ToolMessage(msg, tool_call_id=tool_call_id)]})\n",
    "\n",
    "\n",
    "@tool\n",
    "def run_editor(state: Annotated[DeepAgentState, InjectedState]) -> str:\n",
    "    \"\"\"\n",
    "    Run the Editor agent to synthesize all research into final report.\n",
    "\n",
    "    Args:\n",
    "        state: Injected agent state\n",
    "\n",
    "    Returns:\n",
    "        Status string\n",
    "    \"\"\"\n",
    "    sub_state: DeepAgentState = {\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"user_id\": state.get(\"user_id\"),\n",
    "        \"thread_id\": state.get(\"thread_id\"),\n",
    "    }\n",
    "    editor_agent.invoke(sub_state)\n",
    "    return \"Editor completed. Final report is written to report.md.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Orchestrator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[write_research_plan, run_researcher, run_editor, cleanup_files],\n",
    "    system_prompt=ORCHESTRATOR_PROMPT,\n",
    "    state_schema=DeepAgentState,\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "orchestrator_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_orchestrator_response(agent, query, user_id=\"user_123\", thread_id=\"default\"):\n",
    "    \"\"\"\n",
    "    Stream orchestrator agent responses with tool call visibility.\n",
    "    \n",
    "    Args:\n",
    "        agent: The orchestrator agent instance\n",
    "        query: User query string\n",
    "        user_id: User ID for file management\n",
    "        thread_id: Session thread ID for memory\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        \"messages\": [HumanMessage(content=query)],\n",
    "        \"user_id\": user_id,\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "    \n",
    "    config = {'configurable': {'thread_id': thread_id, 'user_id': user_id}}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for chunk in agent.stream(state, stream_mode='messages', config=config):\n",
    "        message = chunk[0] if isinstance(chunk, tuple) else chunk\n",
    "        \n",
    "        if isinstance(message, AIMessage) and message.tool_calls:\n",
    "            for tool_call in message.tool_calls:\n",
    "                print(f\"\\n  Tool Called: {tool_call['name']}\")\n",
    "                print(f\"   Args: {tool_call['args']}\")\n",
    "                print()\n",
    "        \n",
    "        elif isinstance(message, ToolMessage):\n",
    "            print(f\"\\n  Tool Result (length: {len(message.content)} chars)\")\n",
    "            print()\n",
    "        \n",
    "        elif isinstance(message, AIMessage) and message.content:\n",
    "            if isinstance(message.content, list):\n",
    "                for content in message.content:\n",
    "                    if isinstance(content, dict) and content.get('type') == 'text':\n",
    "                        print(content.get('text', ''), end='', flush=True)\n",
    "            else:\n",
    "                print(message.content, end='', flush=True)\n",
    "    \n",
    "    print(f\"\\n\\n{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple question - direct answer\n",
    "stream_orchestrator_response(\n",
    "    orchestrator_agent,\n",
    "    \"What is a 10-K report?\",\n",
    "    user_id=\"user_123\",\n",
    "    thread_id=\"thread_001\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex financial research\n",
    "stream_orchestrator_response(\n",
    "    orchestrator_agent,\n",
    "    \"Do a detailed analysis of Apple's financial performance in 2023 and 2024\",\n",
    "    user_id=\"user_123\",\n",
    "    thread_id=\"thread_apple_001\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-company comparison\n",
    "stream_orchestrator_response(\n",
    "    orchestrator_agent,\n",
    "    \"Compare Amazon and Google's profitability in 2023 and current stock performance\",\n",
    "    user_id=\"user_123\",\n",
    "    thread_id=\"thread_compare_001\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
