{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep AI Finance Researcher\n",
    "\n",
    "Hierarchical multi-agent system with Orchestrator, Researcher, and Editor agents for deep financial research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from typing import Annotated\n",
    "import sqlite3\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from scripts.rag_tools import hybrid_search, live_finance_researcher\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage, ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool, InjectedToolCallId\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from langgraph.types import Command\n",
    "\n",
    "from scripts.file_tools import (\n",
    "    DeepAgentState,\n",
    "    ls,\n",
    "    read_file,\n",
    "    write_file,\n",
    "    cleanup_files,\n",
    "    generate_hash,\n",
    "    _disk_path\n",
    ")\n",
    "\n",
    "from scripts.prompts import (\n",
    "    ORCHESTRATOR_PROMPT,\n",
    "    RESEARCHER_PROMPT,\n",
    "    EDITOR_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatGoogleGenerativeAI(model='gemini-2.5-flash', temperature=0.7)/\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-3-flash-preview')\n",
    "\n",
    "conn = sqlite3.connect(\"data/deep_finance_researcher.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn=conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Worker Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Researcher Agent - uses RAG and live finance tools\n",
    "researcher_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[ls, write_file, read_file, hybrid_search, live_finance_researcher],\n",
    "    system_prompt=RESEARCHER_PROMPT,\n",
    "    state_schema=DeepAgentState,\n",
    ")\n",
    "\n",
    "# Editor Agent\n",
    "editor_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[ls, read_file, write_file, cleanup_files],\n",
    "    system_prompt=EDITOR_PROMPT,\n",
    "    state_schema=DeepAgentState,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Orchestrator Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def run_researcher(\n",
    "            theme_id: int,\n",
    "            thematic_question: str,\n",
    "            state: Annotated[DeepAgentState, InjectedState],\n",
    "            max_retries: int = 2\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Run a single Research agent for ONE thematic question.\n",
    "\n",
    "    Args:\n",
    "        theme_id: The theme number (1, 2, 3, etc.)\n",
    "        thematic_question: The specific thematic question to research\n",
    "        state: Injected agent state\n",
    "        max_retries: Number of retry attempts\n",
    "\n",
    "    Returns:\n",
    "        Status string for the orchestrator\n",
    "    \"\"\"\n",
    "    \n",
    "    file_hash = generate_hash(f\"{theme_id}_{thematic_question}\")\n",
    "\n",
    "    ai_message_instruction = f\"\"\"[THEME {theme_id}] {thematic_question}\n",
    "\n",
    "                        Save research to: researcher/{file_hash}_theme.md\n",
    "                        Save sources to: researcher/{file_hash}_sources.txt\n",
    "                        \"\"\"\n",
    "\n",
    "    sub_state: DeepAgentState = {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(ai_message_instruction)],\n",
    "        \"user_id\": state.get(\"user_id\"),\n",
    "        \"thread_id\": state.get(\"thread_id\"),\n",
    "    }\n",
    "\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            researcher_agent.invoke(sub_state)\n",
    "            return f\"✓ Theme {theme_id} research completed (hash: {file_hash})\"\n",
    "        except Exception:\n",
    "            print(f\"Failed. Trying #{attempt} times\")\n",
    "\n",
    "    return f\"✗ Theme {theme_id} failed after {max_retries + 1} attempts\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def write_research_plan(\n",
    "    thematic_questions: list[str],\n",
    "    state: Annotated[DeepAgentState, InjectedState],\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "):\n",
    "    \"\"\"\n",
    "    Write the high-level research plan with major thematic questions.\n",
    "\n",
    "    Args:\n",
    "        thematic_questions: List of 3-5 major thematic questions\n",
    "        state: Injected agent state\n",
    "        tool_call_id: Tool call ID\n",
    "\n",
    "    Returns:\n",
    "        Command with ToolMessage confirming the plan was written\n",
    "    \"\"\"\n",
    "    content = \"# Research Plan\\n\\n\"\n",
    "\n",
    "    content = content + \"## User Query\\n\"\n",
    "    content = content + state[\"messages\"][-1].text + \"\\n\\n\"\n",
    "\n",
    "    content = content + \"## Thematic Questions\\n\\n\"\n",
    "    for i, question in enumerate(thematic_questions, 1):\n",
    "        content = content + f\"{i}. {question}\\n\"\n",
    "\n",
    "    path = _disk_path(state, \"research_plan.md\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "    msg = f\"[RESEARCH PLAN WRITTEN] research_plan.md with {len(thematic_questions)} thematic questions\"\n",
    "    return Command(update={\"messages\": [ToolMessage(msg, tool_call_id=tool_call_id)]})\n",
    "\n",
    "\n",
    "@tool\n",
    "def run_editor(state: Annotated[DeepAgentState, InjectedState]) -> str:\n",
    "    \"\"\"\n",
    "    Run the Editor agent to synthesize all research into final report.\n",
    "\n",
    "    Args:\n",
    "        state: Injected agent state\n",
    "\n",
    "    Returns:\n",
    "        Status string\n",
    "    \"\"\"\n",
    "    sub_state: DeepAgentState = {\n",
    "        \"messages\": [HumanMessage(content=\"Read research_plan.md and all files in the researcher/ folder, then synthesize everything into a comprehensive report.md file.\")],\n",
    "        \"user_id\": state.get(\"user_id\"),\n",
    "        \"thread_id\": state.get(\"thread_id\"),\n",
    "    }\n",
    "    editor_agent.invoke(sub_state)\n",
    "    return \"Editor completed. Final report is written to report.md.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Orchestrator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[write_research_plan, run_researcher, run_editor, cleanup_files],\n",
    "    system_prompt=ORCHESTRATOR_PROMPT,\n",
    "    state_schema=DeepAgentState,\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "orchestrator_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.agent_utils import stream_agent_response\n",
    "\n",
    "# Simple question - direct answer\n",
    "stream_agent_response(\n",
    "    orchestrator_agent,\n",
    "    \"What is a 10-K report?\",\n",
    "    thread_id=\"thread_001\",\n",
    "        user_id=\"user_123\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex financial research\n",
    "stream_agent_response(\n",
    "    orchestrator_agent,\n",
    "    \"Do a detailed analysis of Apple's financial performance in 2023 and 2024\",\n",
    "    thread_id=\"thread_apple_001\",\n",
    "    user_id=\"user_123\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = editor_agent.invoke(\n",
    "#     {'messages':[HumanMessage('write detailed report in report.md')],\n",
    "#      'user_id':\"user_123\",\n",
    "#     'thread_id':\"thread_apple_001\"\n",
    "#      }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
