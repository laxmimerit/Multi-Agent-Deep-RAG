{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced RAG - Data Extraction Pipeline\n",
    "### Extract PDFs to Markdown, Images, and Tables\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Extract PDF content to markdown format\n",
    "- Save all figures as PNG images\n",
    "- Extract tables with context (2 paragraphs before)\n",
    "- Organize extracted content systematically\n",
    "\n",
    "**The Complete Workflow:**\n",
    "\n",
    "This is the **first step** in our Multi-Agent Deep RAG pipeline:\n",
    "\n",
    "**Step 1: Data Extraction (This Notebook - 06-01)**\n",
    "- Extract raw PDFs into structured formats\n",
    "- Convert PDFs to markdown with page breaks\n",
    "- Save full page images when large figures (>500x500) are detected\n",
    "- Extract tables with 2 paragraphs of context for better understanding\n",
    "- Organize by company and document\n",
    "\n",
    "**Step 2: Data Ingestion (06-02)**\n",
    "- Load extracted markdown files\n",
    "- Split by page breaks for granular chunks\n",
    "- Add metadata (company, doc_type, fiscal_year, fiscal_quarter, page)\n",
    "- Create embeddings (dense + sparse for hybrid search)\n",
    "- Store in Qdrant vector database with deduplication\n",
    "\n",
    "**Step 3: Retrieval (07)**\n",
    "- Dynamic filter extraction from user queries using LLM\n",
    "- Hybrid search (dense semantic + sparse keyword matching)\n",
    "- Reranking with cross-encoder for relevance\n",
    "- Return top results with metadata for context\n",
    "\n",
    "**Why This Approach?**\n",
    "- **Markdown**: Preserves structure, easy to chunk, searchable\n",
    "- **Page Images**: Full context for charts/diagrams with titles/headers\n",
    "- **Tables with Context**: Include descriptions and captions\n",
    "- **Metadata**: Enables precise filtering (company, year, quarter, doc type)\n",
    "\n",
    "**Output Structure:**\n",
    "- Markdown: `data/rag-markdown/{company}/{filename}.md`\n",
    "- Images: `data/rag-images/{company}/{filename}/page_X.png`\n",
    "- Tables: `data/rag-tables/{company}/{filename}/table_X.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Multi-Agent Deep RAG Pipeline - Complete Workflow\n\n**Overview:**\nThis course demonstrates a production-ready RAG system with three distinct stages: Extraction, Ingestion, and Retrieval.\n\n---\n\n### **Notebook 06-01: Data Extraction (Current)**\n**Purpose:** Convert raw PDFs into structured, searchable formats\n\n**What We Do:**\n1. **PDF → Markdown Conversion**\n   - Extract full document text with preserved structure\n   - Insert page breaks (`<!-- page break -->`) for chunking\n   - Save as `.md` files organized by company\n\n2. **Intelligent Image Extraction**\n   - Detect large images (>500x500 pixels) in PDFs\n   - Save **entire page** as PNG when large image found\n   - Preserves titles, headers, and context around charts/diagrams\n   - Avoids cropping issues with individual image extraction\n\n3. **Table Extraction with Context**\n   - Identify markdown tables in text\n   - Extract 2 paragraphs **before** each table\n   - Ensures table titles and descriptions are included\n   - Save as separate `.md` files for targeted retrieval\n\n**Output:**\n```\ndata/rag-markdown/{company}/{document}.md\ndata/rag-images/{company}/{document}/page_5.png\ndata/rag-tables/{company}/{document}/table_1.md\n```\n\n---\n\n### **Notebook 06-02: Data Ingestion**\n**Purpose:** Load extracted data into vector database for semantic search\n\n**What We Do:**\n1. **Load Markdown Files**\n   - Read extracted markdown from 06-01\n   - Split by page breaks for page-level chunks\n\n2. **Metadata Enrichment**\n   - Extract from filename: company, doc_type, fiscal_year, fiscal_quarter\n   - Add page numbers, file hash (for deduplication)\n   - Attach metadata to each chunk\n\n3. **Hybrid Embeddings**\n   - **Dense vectors**: Gemini embeddings (semantic understanding)\n   - **Sparse vectors**: BM25 (keyword matching)\n   - Store both in Qdrant for hybrid retrieval\n\n4. **Deduplication**\n   - SHA-256 hash-based duplicate detection\n   - Skip already processed files\n\n**Output:**\n- Qdrant collection with 1000+ page chunks\n- Each chunk has dense + sparse embeddings\n- Rich metadata for filtering\n\n---\n\n### **Notebook 07: Retrieval**\n**Purpose:** Intelligent search with dynamic filtering and reranking\n\n**What We Do:**\n1. **Dynamic Filter Extraction**\n   - User query: \"Amazon Q1 2024 revenue\"\n   - LLM extracts: `{company: \"amazon\", fiscal_year: 2024, fiscal_quarter: \"q1\"}`\n   - Automatic company name mapping (AMZN → amazon)\n   - Document type mapping (annual report → 10-k)\n\n2. **Hybrid Search**\n   - Search filtered subset (e.g., only Amazon Q1 2024)\n   - Dense + sparse vectors combined with RRF/DBSF fusion\n   - Fetch top-k candidates (e.g., 10 results)\n\n3. **Reranking**\n   - Cross-encoder (BAAI/bge-reranker-base) scores relevance\n   - Reorder results by true semantic similarity\n   - Return top-n (e.g., 5 best matches)\n\n4. **Context-Rich Results**\n   - Return page text with metadata\n   - Include page images if available\n   - Reference tables with context\n\n**Why This Works:**\n- **Filters reduce search space** → faster, more precise\n- **Hybrid search** → catches both semantic + keyword matches\n- **Reranking** → ensures best results at top\n- **Metadata** → enables multi-dimensional filtering\n\n---\n\n### **Key Design Decisions**\n\n**Why Page-Level Chunking?**\n- Financial docs have page-specific info (page numbers cited in discussions)\n- Easier to reference and verify sources\n- Natural boundary for context\n\n**Why Save Full Page Images?**\n- Charts/tables often lack context when cropped individually\n- Titles and captions usually appear above/beside figures\n- Full page preserves visual layout\n\n**Why Extract Tables Separately?**\n- Tables are high-value structured data\n- With context (2 paragraphs), they're self-contained\n- Can be used for targeted table-based RAG\n\n**Why Hybrid Search?**\n- Dense: \"What's the company's profitability?\" (semantic)\n- Sparse: \"EBITDA margin Q3\" (exact terms)\n- Together: Best of both worlds\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "from docling_core.types.doc import PictureItem\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = \"data/rag-data/rag-pdf\"\n",
    "OUTPUT_MD_DIR = \"data/rag-data/rag-markdown\"\n",
    "OUTPUT_IMAGES_DIR = \"data/rag-data/rag-images\"\n",
    "OUTPUT_TABLES_DIR = \"data/rag-data/rag-tables\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Metadata from Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_from_filename(filename: str) -> dict:\n",
    "    \"\"\"Extract metadata from filename.\"\"\"\n",
    "    name = filename.replace('.pdf', '')\n",
    "    parts = name.split()\n",
    "    \n",
    "    metadata = {}\n",
    "    \n",
    "    # Handle different filename patterns\n",
    "    if len(parts) >= 3:\n",
    "        metadata['company_name'] = parts[0]\n",
    "        metadata['doc_type'] = parts[1]\n",
    "        \n",
    "        if len(parts) == 4:\n",
    "            metadata['fiscal_quarter'] = parts[2]\n",
    "            metadata['fiscal_year'] = int(parts[3])\n",
    "        elif len(parts) == 3:\n",
    "            metadata['fiscal_quarter'] = None\n",
    "            metadata['fiscal_year'] = int(parts[2])\n",
    "        else:\n",
    "            # More than 4 parts - use last as year\n",
    "            metadata['fiscal_quarter'] = parts[2] if len(parts) > 3 else None\n",
    "            metadata['fiscal_year'] = int(parts[-1])\n",
    "    else:\n",
    "        # Fallback for non-standard filenames\n",
    "        metadata['company_name'] = parts[0] if parts else 'unknown'\n",
    "        metadata['doc_type'] = parts[1] if len(parts) > 1 else 'unknown'\n",
    "        metadata['fiscal_quarter'] = None\n",
    "        metadata['fiscal_year'] = None\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Tables with Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_with_context(markdown_text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract tables with 2 paragraphs of context before each table.\n",
    "    \n",
    "    Returns:\n",
    "        List of (context + table, table_number) tuples\n",
    "    \"\"\"\n",
    "    # Split by table pattern (markdown tables start with |)\n",
    "    lines = markdown_text.split('\\n')\n",
    "    \n",
    "    tables = []\n",
    "    i = 0\n",
    "    table_num = 1\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        \n",
    "        # Detect table start (line with multiple |)\n",
    "        if line.strip().startswith('|') and line.count('|') >= 2:\n",
    "            # Find 2 paragraphs before\n",
    "            context_lines = []\n",
    "            para_count = 0\n",
    "            j = i - 1\n",
    "            \n",
    "            while j >= 0 and para_count < 2:\n",
    "                if lines[j].strip():  # Non-empty line\n",
    "                    context_lines.insert(0, lines[j])\n",
    "                elif context_lines:  # Empty line marks paragraph break\n",
    "                    para_count += 1\n",
    "                j -= 1\n",
    "            \n",
    "            # Extract full table\n",
    "            table_lines = []\n",
    "            while i < len(lines) and (lines[i].strip().startswith('|') or not lines[i].strip()):\n",
    "                if lines[i].strip():  # Skip empty lines within table\n",
    "                    table_lines.append(lines[i])\n",
    "                i += 1\n",
    "                if i < len(lines) and lines[i].strip() and not lines[i].strip().startswith('|'):\n",
    "                    break\n",
    "            \n",
    "            # Combine context + table\n",
    "            full_content = '\\n'.join(context_lines) + '\\n\\n' + '\\n'.join(table_lines)\n",
    "            tables.append((full_content, f\"table_{table_num}\"))\n",
    "            table_num += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract PDF Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_content(pdf_path: Path):\n",
    "    \"\"\"Extract PDF to markdown, images, and tables.\"\"\"\n",
    "    print(f\"Processing: {pdf_path.name}\")\n",
    "    \n",
    "    # Get metadata and create directories\n",
    "    metadata = extract_metadata_from_filename(pdf_path.name)\n",
    "    company = metadata['company_name']\n",
    "    filename_stem = pdf_path.stem\n",
    "    \n",
    "    md_dir = Path(OUTPUT_MD_DIR) / company\n",
    "    images_dir = Path(OUTPUT_IMAGES_DIR) / company / filename_stem\n",
    "    tables_dir = Path(OUTPUT_TABLES_DIR) / company / filename_stem\n",
    "    \n",
    "    for dir_path in [md_dir, images_dir, tables_dir]:\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Configure and convert\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.generate_picture_images = True\n",
    "    pipeline_options.generate_page_images = True\n",
    "    \n",
    "    converter = DocumentConverter(\n",
    "        format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}\n",
    "    )\n",
    "    result = converter.convert(str(pdf_path))\n",
    "    \n",
    "    # Save markdown\n",
    "    page_break = \"<!-- page break -->\"\n",
    "    markdown_text = result.document.export_to_markdown(page_break_placeholder=page_break)\n",
    "    (md_dir / f\"{filename_stem}.md\").write_text(markdown_text, encoding='utf-8')\n",
    "    print(f\"  ✓ Markdown saved\")\n",
    "    \n",
    "    # Find pages with large images and save them\n",
    "    pages_to_save = set()\n",
    "    for element, _ in result.document.iterate_items():\n",
    "        if isinstance(element, PictureItem):\n",
    "            image = element.get_image(result.document)\n",
    "            if image.size[0] > 500 and image.size[1] > 500:\n",
    "                page_no = element.prov[0].page_no if element.prov else None\n",
    "                if page_no:\n",
    "                    pages_to_save.add(page_no)\n",
    "    \n",
    "    # Save page images\n",
    "    for page_no in pages_to_save:\n",
    "        page = result.document.pages[page_no]\n",
    "        page.image.pil_image.save(images_dir / f\"page_{page_no}.png\", \"PNG\")\n",
    "    \n",
    "    if pages_to_save:\n",
    "        print(f\"  ✓ Saved {len(pages_to_save)} page images\")\n",
    "    \n",
    "    # Save tables with context\n",
    "    tables = extract_tables_with_context(markdown_text)\n",
    "    for table_content, table_name in tables:\n",
    "        (tables_dir / f\"{table_name}.md\").write_text(table_content, encoding='utf-8')\n",
    "    \n",
    "    if tables:\n",
    "        print(f\"  ✓ Saved {len(tables)} tables\")\n",
    "    \n",
    "    print(f\"  [DONE]\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process All PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all PDF files\n",
    "data_path = Path(DATA_DIR)\n",
    "pdf_files = list(data_path.rglob(\"*.pdf\"))\n",
    "print(f\"Found {len(pdf_files)} PDF files\\n\")\n",
    "\n",
    "# Process each PDF\n",
    "for pdf_path in pdf_files:\n",
    "    extract_pdf_content(pdf_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}