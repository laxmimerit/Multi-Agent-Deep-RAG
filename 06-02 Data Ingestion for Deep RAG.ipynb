{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Ingestion for Deep RAG\n",
        "\n",
        "In this notebook, we'll load extracted data into Qdrant vector database:\n",
        "\n",
        "- **Markdown**: Page-level chunks with metadata\n",
        "- **Tables**: Separate documents with context and page numbers\n",
        "- **Images**: Multimodal embeddings for visual content\n",
        "- **Hybrid Search**: Dense (semantic) + Sparse (keyword) embeddings\n",
        "\n",
        "**Prerequisites:**\n",
        "- Run notebook 06-01 first to extract PDFs\n",
        "- Qdrant server running on localhost:6333\n",
        "\n",
        "**Output:**\n",
        "- Single Qdrant collection with all content types\n",
        "- Rich metadata for filtering (company, year, quarter, doc_type, page)\n",
        "- Deduplication using file hashes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "from langchain_qdrant import QdrantVectorStore, RetrievalMode, FastEmbedSparse\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "MARKDOWN_DIR = \"data/rag-data/markdown\"\n",
        "TABLES_DIR = \"data/rag-data/tables\"\n",
        "IMAGES_DIR = \"data/rag-data/images\"\n",
        "\n",
        "# Qdrant Configuration\n",
        "COLLECTION_NAME = \"financial_docs\"\n",
        "EMBEDDING_MODEL = \"multimodalembedding@001\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Initialize Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multimodal embeddings (Vertex AI) - works for text AND images\n",
        "embeddings = VertexAIEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "\n",
        "# Sparse embeddings (BM25 for keyword matching)\n",
        "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
        "\n",
        "# Initialize vector store with hybrid retrieval\n",
        "vector_store = QdrantVectorStore.from_documents(\n",
        "    documents=[],\n",
        "    embedding=embeddings,\n",
        "    sparse_embedding=sparse_embeddings,\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    url=\"http://localhost:6333\",\n",
        "    retrieval_mode=RetrievalMode.HYBRID,\n",
        "    force_recreate=True\n",
        ")\n",
        "\n",
        "print(f\"✓ Vector store initialized: {COLLECTION_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_metadata_from_filename(filename: str) -> dict:\n",
        "    \"\"\"\n",
        "    Extract metadata from filename.\n",
        "    \n",
        "    Expected format: CompanyName DocType [Quarter] Year.md\n",
        "    Examples:\n",
        "        - Amazon 10-K 2024.md\n",
        "        - Amazon 10-Q Q1 2024.md\n",
        "    \"\"\"\n",
        "    name = filename.replace('.md', '').replace('.pdf', '')\n",
        "    parts = name.split()\n",
        "    \n",
        "    return {\n",
        "        'company_name': parts[0],\n",
        "        'doc_type': parts[1],\n",
        "        'fiscal_quarter': parts[2] if len(parts) == 4 else None,\n",
        "        'fiscal_year': int(parts[-1])\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_file_hash(file_path: Path) -> str:\n",
        "    \"\"\"Compute SHA-256 hash for deduplication.\"\"\"\n",
        "    sha256_hash = hashlib.sha256()\n",
        "    with open(file_path, 'rb') as f:\n",
        "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
        "            sha256_hash.update(byte_block)\n",
        "    return sha256_hash.hexdigest()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_processed_hashes() -> set:\n",
        "    \"\"\"Get file hashes already in Qdrant.\"\"\"\n",
        "    all_points = vector_store.client.scroll(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        limit=10000,\n",
        "        with_payload=True\n",
        "    )\n",
        "    \n",
        "    hashes = set(\n",
        "        point.payload.get('file_hash') \n",
        "        for point in all_points[0] \n",
        "        if point.payload.get('file_hash')\n",
        "    )\n",
        "    \n",
        "    print(f\"Already processed: {len(hashes)} files\")\n",
        "    return hashes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Ingestion Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ingest_markdown_file(md_path: Path, processed_hashes: set):\n",
        "    \"\"\"Ingest markdown file split by pages.\"\"\"\n",
        "    file_hash = compute_file_hash(md_path)\n",
        "    if file_hash in processed_hashes:\n",
        "        print(f\"  [SKIP] {md_path.name}\")\n",
        "        return 0\n",
        "    \n",
        "    # Read and split by page breaks\n",
        "    markdown_text = md_path.read_text(encoding='utf-8')\n",
        "    pages = markdown_text.split(\"<!-- page break -->\")\n",
        "    \n",
        "    # Get metadata from filename\n",
        "    file_metadata = extract_metadata_from_filename(md_path.name)\n",
        "    \n",
        "    # Create documents for each page\n",
        "    documents = []\n",
        "    for page_num, page_text in enumerate(pages, start=1):\n",
        "        if page_text.strip():\n",
        "            metadata = file_metadata.copy()\n",
        "            metadata['content_type'] = 'text'\n",
        "            metadata['page'] = page_num\n",
        "            metadata['file_hash'] = file_hash\n",
        "            metadata['source_file'] = md_path.name\n",
        "            \n",
        "            documents.append(Document(page_content=page_text.strip(), metadata=metadata))\n",
        "    \n",
        "    # Add to vector store\n",
        "    if documents:\n",
        "        vector_store.add_documents(documents=documents)\n",
        "        processed_hashes.add(file_hash)\n",
        "        print(f\"  ✓ {md_path.name} ({len(documents)} pages)\")\n",
        "    \n",
        "    return len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ingest_table_file(table_path: Path, doc_name: str, processed_hashes: set):\n",
        "    \"\"\"\n",
        "    Ingest a single table file.\n",
        "    \n",
        "    Args:\n",
        "        table_path: Path to table file (e.g., table_1_page_5.md)\n",
        "        doc_name: Parent document name for metadata\n",
        "        processed_hashes: Set of already processed file hashes\n",
        "    \"\"\"\n",
        "    file_hash = compute_file_hash(table_path)\n",
        "    if file_hash in processed_hashes:\n",
        "        return 0\n",
        "    \n",
        "    # Read table content\n",
        "    table_content = table_path.read_text(encoding='utf-8')\n",
        "    \n",
        "    # Extract metadata from filename\n",
        "    file_metadata = extract_metadata_from_filename(doc_name + '.md')\n",
        "    \n",
        "    # Extract table number and page number from filename\n",
        "    # Format: table_1_page_5.md\n",
        "    stem = table_path.stem  # table_1_page_5\n",
        "    parts = stem.split('_')\n",
        "    table_num = int(parts[1])  # 1\n",
        "    page_num = int(parts[3]) if len(parts) >= 4 else None  # 5\n",
        "    \n",
        "    # Create metadata\n",
        "    metadata = file_metadata.copy()\n",
        "    metadata['content_type'] = 'table'\n",
        "    metadata['table_number'] = table_num\n",
        "    metadata['page'] = page_num\n",
        "    metadata['file_hash'] = file_hash\n",
        "    metadata['source_file'] = table_path.name\n",
        "    \n",
        "    # Add to vector store\n",
        "    doc = Document(page_content=table_content, metadata=metadata)\n",
        "    vector_store.add_documents([doc])\n",
        "    processed_hashes.add(file_hash)\n",
        "    \n",
        "    return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ingest_image_file(image_path: Path, doc_name: str, processed_hashes: set):\n",
        "    \"\"\"\n",
        "    Ingest a single image file with multimodal embeddings.\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to image file (e.g., page_5.png)\n",
        "        doc_name: Parent document name for metadata\n",
        "        processed_hashes: Set of already processed file hashes\n",
        "    \"\"\"\n",
        "    file_hash = compute_file_hash(image_path)\n",
        "    if file_hash in processed_hashes:\n",
        "        return 0\n",
        "    \n",
        "    # Extract page number from filename (page_5.png)\n",
        "    page_num = int(image_path.stem.split('_')[1])\n",
        "    \n",
        "    # Extract metadata from parent document name\n",
        "    file_metadata = extract_metadata_from_filename(doc_name + '.md')\n",
        "    \n",
        "    # Create metadata\n",
        "    metadata = file_metadata.copy()\n",
        "    metadata['content_type'] = 'image'\n",
        "    metadata['page'] = page_num\n",
        "    metadata['image_path'] = str(image_path)\n",
        "    metadata['file_hash'] = file_hash\n",
        "    \n",
        "    # Embed image using multimodal embeddings\n",
        "    image_embedding = embeddings.embed_image(uri=str(image_path))\n",
        "    \n",
        "    # Create document\n",
        "    doc = Document(\n",
        "        page_content=f\"Visual content from page {page_num}\",\n",
        "        metadata=metadata\n",
        "    )\n",
        "    \n",
        "    # Add with custom embedding\n",
        "    vector_store.add_embeddings([(doc, image_embedding)])\n",
        "    processed_hashes.add(file_hash)\n",
        "    \n",
        "    return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ingest_company_tables(company_dir: Path, processed_hashes: set) -> int:\n",
        "    \"\"\"Ingest all tables for a company.\"\"\"\n",
        "    table_count = 0\n",
        "    \n",
        "    for doc_dir in company_dir.iterdir():\n",
        "        if doc_dir.is_dir():\n",
        "            for table_file in doc_dir.glob(\"table_*.md\"):\n",
        "                table_count += ingest_table_file(table_file, doc_dir.name, processed_hashes)\n",
        "    \n",
        "    return table_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ingest_company_images(company_dir: Path, processed_hashes: set) -> int:\n",
        "    \"\"\"Ingest all images for a company.\"\"\"\n",
        "    image_count = 0\n",
        "    \n",
        "    for doc_dir in company_dir.iterdir():\n",
        "        if doc_dir.is_dir():\n",
        "            for image_file in doc_dir.glob(\"page_*.png\"):\n",
        "                image_count += ingest_image_file(image_file, doc_dir.name, processed_hashes)\n",
        "    \n",
        "    return image_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Process All Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get already processed files\n",
        "processed_hashes = get_processed_hashes()\n",
        "\n",
        "# Process markdown files\n",
        "print(\"\\n=== Ingesting Markdown Files ===\")\n",
        "markdown_path = Path(MARKDOWN_DIR)\n",
        "md_files = list(markdown_path.rglob(\"*.md\"))\n",
        "print(f\"Found {len(md_files)} markdown files\\n\")\n",
        "\n",
        "total_pages = 0\n",
        "for idx, md_path in enumerate(md_files, 1):\n",
        "    print(f\"[{idx}/{len(md_files)}]\", end=\" \")\n",
        "    total_pages += ingest_markdown_file(md_path, processed_hashes)\n",
        "\n",
        "print(f\"\\nTotal pages ingested: {total_pages}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process tables\n",
        "print(\"\\n=== Ingesting Tables ===\")\n",
        "tables_path = Path(TABLES_DIR)\n",
        "company_dirs = [d for d in tables_path.iterdir() if d.is_dir()]\n",
        "print(f\"Found {len(company_dirs)} companies\\n\")\n",
        "\n",
        "total_tables = 0\n",
        "for idx, company_dir in enumerate(company_dirs, 1):\n",
        "    print(f\"[{idx}/{len(company_dirs)}] {company_dir.name}...\", end=\" \")\n",
        "    count = ingest_company_tables(company_dir, processed_hashes)\n",
        "    total_tables += count\n",
        "    print(f\"✓ {count} tables\")\n",
        "\n",
        "print(f\"\\nTotal tables ingested: {total_tables}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process images\n",
        "print(\"\\n=== Ingesting Images ===\")\n",
        "images_path = Path(IMAGES_DIR)\n",
        "company_dirs = [d for d in images_path.iterdir() if d.is_dir()]\n",
        "print(f\"Found {len(company_dirs)} companies\\n\")\n",
        "\n",
        "total_images = 0\n",
        "for idx, company_dir in enumerate(company_dirs, 1):\n",
        "    print(f\"[{idx}/{len(company_dirs)}] {company_dir.name}...\", end=\" \")\n",
        "    count = ingest_company_images(company_dir, processed_hashes)\n",
        "    total_images += count\n",
        "    print(f\"✓ {count} images\")\n",
        "\n",
        "print(f\"\\nTotal images ingested: {total_images}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Verify Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "collection_info = vector_store.client.get_collection(COLLECTION_NAME)\n",
        "print(f\"\\n=== Collection Summary ===\")\n",
        "print(f\"Total documents: {collection_info.points_count}\")\n",
        "print(f\"Vector size: {collection_info.config.params.vectors.size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Test Hybrid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test hybrid search\n",
        "query = \"What is Amazon's revenue?\"\n",
        "results = vector_store.similarity_search(query, k=5)\n",
        "\n",
        "print(f\"Query: {query}\\n\")\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"{i}. Type: {doc.metadata.get('content_type')} | Page: {doc.metadata.get('page')}\")\n",
        "    print(f\"   Company: {doc.metadata.get('company_name')} | Year: {doc.metadata.get('fiscal_year')}\")\n",
        "    print(f\"   Content: {doc.page_content[:150]}...\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
